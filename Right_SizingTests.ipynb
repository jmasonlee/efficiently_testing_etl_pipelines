{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jmasonlee/efficiently_testing_etl_pipelines/blob/main/Right_SizingTests.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Efficiencies of Right Sizing Tests\n"
      ],
      "metadata": {
        "id": "Bhc_PE_4BrpG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This exercise is meant to give you a practical example on how the number of inputs to a test affect the number of tests you need to write and maintain in order to fully cover your system."
      ],
      "metadata": {
        "id": "qljkG08NDynH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup Notebook"
      ],
      "metadata": {
        "id": "aqYjZpizVD3i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf efficiently_testing_etl_pipelines\n",
        "!git clone https://github.com/jmasonlee/efficiently_testing_etl_pipelines.git\n",
        "!cp /content/efficiently_testing_etl_pipelines/efficiencies_of_right_sizing_tests/src/diamond_pricing.py .\n",
        "!cp /content/efficiently_testing_etl_pipelines/efficiencies_of_right_sizing_tests/tests/test_helpers/notebook_verification_helpers.py .\n",
        "!rm -rf efficiently_testing_etl_pipelines\n",
        "!rm -rf sample_data\n"
      ],
      "metadata": {
        "id": "i4KRAp6vTEJm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Install Dependencies\n",
        "\n",
        "For the exercise, we will need some special dependencies to allow us to run lots of tests in a notebook."
      ],
      "metadata": {
        "id": "_h199h_XEAwT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "`ipytest` lets us run our tests in a notebook.\n",
        "\n"
      ],
      "metadata": {
        "id": "DVAcSonZmslt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ipytest"
      ],
      "metadata": {
        "id": "7v0kaWulDXi-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ipytest is what allows us to run our tests in a notebook. This next cell is not needed if you are writing tests in a separate pytest file."
      ],
      "metadata": {
        "id": "3_ILjU8goCNY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import ipytest\n",
        "ipytest.autoconfig()"
      ],
      "metadata": {
        "id": "t7iwGQ4Le-Oe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We are installing `pyspark` because it doesn't come with the base colab environment"
      ],
      "metadata": {
        "id": "HzwfHFGmm2Ca"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark"
      ],
      "metadata": {
        "id": "QNnk84AsmmFQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "approvaltests is what lets us run our tests with many combinations"
      ],
      "metadata": {
        "id": "pQRzoDwjnFKb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install approvaltests"
      ],
      "metadata": {
        "id": "TpAnrxqKmqQ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create a local SparkSession\n",
        "\n",
        "Normally spark runs on a bunch of executors in the cloud. Since we want our tests to be able to run on a single dev machine, we make a fixture that gives us a local spark context."
      ],
      "metadata": {
        "id": "H2on72wxnQqS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pytest\n",
        "from _pytest.fixtures import FixtureRequest\n",
        "from pyspark import SparkConf\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "@pytest.fixture(scope=\"session\")\n",
        "def spark(request: FixtureRequest):\n",
        "    conf = (SparkConf()\n",
        "        .setMaster(\"local\")\n",
        "        .setAppName(\"sample_pyspark_testing_starter\"))\n",
        "\n",
        "    spark = SparkSession \\\n",
        "        .builder \\\n",
        "        .config(conf=conf) \\\n",
        "        .getOrCreate()\n",
        "\n",
        "    request.addfinalizer(lambda: spark.stop())\n",
        "    return spark"
      ],
      "metadata": {
        "id": "V6XwgiHHJrYr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise"
      ],
      "metadata": {
        "id": "TATb7BjoCXP6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is a test for a piece of code that replaces all null values in the price column of a dataframe with an average price.\n",
        "\n",
        "The average price is calculated from the price of other diamonds with the same cut, clarity and color."
      ],
      "metadata": {
        "id": "zy1yGzENr3RB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Initial State (1 test)\n",
        "\n",
        "Run the below cell contining our first test. The test will fail. What does the failure look like here?"
      ],
      "metadata": {
        "id": "2PNO9pc4v036"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%ipytest -qq\n",
        "from diamond_pricing import replace_null_prices_with_floating_averages\n",
        "from notebook_verification_helpers import verify_will_replace_null_values_with_floating_averages\n",
        "\n",
        "def test_will_replace_null_prices_with_floating_averages(spark: SparkSession) -> None:\n",
        "    price = [327]\n",
        "\n",
        "    verify_will_replace_null_values_with_floating_averages(spark, price)\n"
      ],
      "metadata": {
        "id": "lfANxIAgIFEz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The test created 2 files. One file name ends in `approved.txt`. The other file name ends in `received.txt`.\n",
        "\n",
        "Look at the `received.txt` file. If it looks good, approve it by running the cell below. Rerun the cell containing the test. It should pass."
      ],
      "metadata": {
        "id": "GWGcu5_4wSsT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mv /content/test_one_test.received.txt /content/test_one_test.approved.txt"
      ],
      "metadata": {
        "id": "dkeXQbvySmES"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Add a Price of None\n",
        "\n",
        "Right now, we only have one test for a diamond with a price of 327. The price is wrapped in an array. Add a new test by adding a new item to that array.\n",
        "\n",
        "Because you are adding a new input, the test will fail."
      ],
      "metadata": {
        "id": "7l_18P3sxkVs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%ipytest -qq\n",
        "from diamond_pricing import replace_null_prices_with_floating_averages\n",
        "from notebook_verification_helpers import verify_will_replace_null_values_with_floating_averages\n",
        "\n",
        "def test_will_replace_null_prices_with_floating_averages(spark: SparkSession) -> None:\n",
        "    price = [327]\n",
        "\n",
        "    verify_will_replace_null_values_with_floating_averages(spark, price)\n"
      ],
      "metadata": {
        "id": "vJhcxCenyork"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compare the `test__will_replace_null_prices_with_floating_averages.received.txt` file to the `test__will_replace_null_prices_with_floating_averages.approved.txt` file. How is it different?\n",
        "\n",
        "Each line in the files represents one test case. You have just created 2 tests. Run the cell below to update the expected output. Re-run the test cell, it should pass."
      ],
      "metadata": {
        "id": "vmDEnhdOzne7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mv /content/test_will_replace_null_prices_with_floating_averages.received.txt /content/test_will_replace_null_prices_with_floating_averages.approved.txt"
      ],
      "metadata": {
        "id": "nW_gsNBgy_jK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}