{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jmasonlee/efficiently_testing_etl_pipelines/blob/main/Right_SizingTests.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Efficiencies of Right Sizing Tests\n"
      ],
      "metadata": {
        "id": "Bhc_PE_4BrpG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This exercise is meant to give you a practical example on how the number of inputs to a test affect the number of tests you need to write and maintain in order to fully cover your system."
      ],
      "metadata": {
        "id": "qljkG08NDynH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup Notebook"
      ],
      "metadata": {
        "id": "aqYjZpizVD3i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf efficiently_testing_etl_pipelines\n",
        "!git clone https://github.com/jmasonlee/efficiently_testing_etl_pipelines.git\n",
        "!cp /content/efficiently_testing_etl_pipelines/efficiencies_of_right_sizing_tests/src/diamond_pricing.py .\n",
        "!cp /content/efficiently_testing_etl_pipelines/efficiencies_of_right_sizing_tests/tests/test_helpers/notebook_verification_helpers.py .\n",
        "!rm -rf efficiently_testing_etl_pipelines\n",
        "!rm -rf sample_data\n"
      ],
      "metadata": {
        "id": "i4KRAp6vTEJm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Install Dependencies\n",
        "\n",
        "For the exercise, we will need some special dependencies to allow us to run lots of tests in a notebook."
      ],
      "metadata": {
        "id": "_h199h_XEAwT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "`ipytest` lets us run our tests in a notebook.\n",
        "\n"
      ],
      "metadata": {
        "id": "DVAcSonZmslt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ipytest"
      ],
      "metadata": {
        "id": "7v0kaWulDXi-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ipytest is what allows us to run our tests in a notebook. This next cell is not needed if you are writing tests in a separate pytest file."
      ],
      "metadata": {
        "id": "3_ILjU8goCNY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import ipytest\n",
        "ipytest.autoconfig()"
      ],
      "metadata": {
        "id": "t7iwGQ4Le-Oe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We are installing `pyspark` because it doesn't come with the base colab environment"
      ],
      "metadata": {
        "id": "HzwfHFGmm2Ca"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark"
      ],
      "metadata": {
        "id": "QNnk84AsmmFQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "approvaltests is what lets us run our tests with many combinations"
      ],
      "metadata": {
        "id": "pQRzoDwjnFKb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install approvaltests"
      ],
      "metadata": {
        "id": "TpAnrxqKmqQ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create a local SparkSession\n",
        "\n",
        "Normally spark runs on a bunch of executors in the cloud. Since we want our tests to be able to run on a single dev machine, we make a fixture that gives us a local spark context."
      ],
      "metadata": {
        "id": "H2on72wxnQqS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pytest\n",
        "from _pytest.fixtures import FixtureRequest\n",
        "from pyspark import SparkConf\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "@pytest.fixture(scope=\"session\")\n",
        "def spark(request: FixtureRequest):\n",
        "    conf = (SparkConf()\n",
        "        .setMaster(\"local\")\n",
        "        .setAppName(\"sample_pyspark_testing_starter\"))\n",
        "\n",
        "    spark = SparkSession \\\n",
        "        .builder \\\n",
        "        .config(conf=conf) \\\n",
        "        .getOrCreate()\n",
        "\n",
        "    request.addfinalizer(lambda: spark.stop())\n",
        "    return spark"
      ],
      "metadata": {
        "id": "V6XwgiHHJrYr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise"
      ],
      "metadata": {
        "id": "TATb7BjoCXP6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is a test for a piece of code that replaces all null values in the price column of a dataframe with an average price.\n",
        "\n",
        "The average price is calculated from the price of other diamonds with the same cut, clarity and color."
      ],
      "metadata": {
        "id": "zy1yGzENr3RB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Initial State (1 test)\n",
        "\n",
        "Run the below cell contining our first test. The test will fail. What does the failure look like here?"
      ],
      "metadata": {
        "id": "2PNO9pc4v036"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%ipytest -qq\n",
        "from diamond_pricing import replace_null_prices_with_floating_averages\n",
        "from notebook_verification_helpers import verify_will_replace_null_values_with_floating_averages\n",
        "\n",
        "def test_will_replace_null_prices_with_floating_averages(spark: SparkSession) -> None:\n",
        "    price = [327]\n",
        "\n",
        "    verify_will_replace_null_values_with_floating_averages(spark, price)\n"
      ],
      "metadata": {
        "id": "lfANxIAgIFEz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The test created 2 files. One file name ends in `approved.txt`. The other file name ends in `received.txt`.\n",
        "\n",
        "Look at the `received.txt` file. If it looks good, approve it by running the cell below. Rerun the cell containing the test. It should pass."
      ],
      "metadata": {
        "id": "GWGcu5_4wSsT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mv /content/test_one_test.received.txt /content/test_one_test.approved.txt"
      ],
      "metadata": {
        "id": "dkeXQbvySmES"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Add a Price of None\n",
        "\n",
        "Right now, we only have one test for a diamond with a price of 327. The price is wrapped in an array. Add a new test by adding a new item to that array.\n",
        "\n",
        "Because you are adding a new input, the test will fail."
      ],
      "metadata": {
        "id": "7l_18P3sxkVs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%ipytest -qq\n",
        "from diamond_pricing import replace_null_prices_with_floating_averages\n",
        "from notebook_verification_helpers import verify_will_replace_null_values_with_floating_averages\n",
        "\n",
        "def test_will_replace_null_prices_with_floating_averages(spark: SparkSession) -> None:\n",
        "    price = [327]\n",
        "\n",
        "    verify_will_replace_null_values_with_floating_averages(spark, price)\n"
      ],
      "metadata": {
        "id": "vJhcxCenyork"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compare the `test__will_replace_null_prices_with_floating_averages.received.txt` file to the `test__will_replace_null_prices_with_floating_averages.approved.txt` file. How is it different?\n",
        "\n",
        "Each line in the files represents one test case. You have just created 2 tests. Run the cell below to update the expected output. Re-run the test cell, it should pass."
      ],
      "metadata": {
        "id": "vmDEnhdOzne7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mv /content/test_will_replace_null_prices_with_floating_averages.received.txt /content/test_will_replace_null_prices_with_floating_averages.approved.txt"
      ],
      "metadata": {
        "id": "nW_gsNBgy_jK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Add a New Input Variable\n",
        "\n",
        "A diamond's cut, clarity or colour can impact it's price.\n",
        "\n",
        "Each of these is measured on it's own scale.\n",
        "\n",
        "* Diamond **[cut](https://www.diamonds.pro/education/cuts/)** can be rated as `\"Poor\", \"Fair\", \"Good\", \"Very Good\"` or `\"Excellent\"`.\n",
        "\n",
        "* Diamond **[clarity](https://www.diamonds.pro/education/clarity/)** can be rated as `\"IF\", \"VVS1\", \"VVS2\", \"VS1\", \"VS2\", \"SI1\", \"SI2\", \"I1\", \"I2\"` or `\"I3\"`\n",
        "\n",
        "* Diamond **[color](https://www.diamonds.pro/education/color/)** is rated on a scale from `D` to `Z`.\n",
        "\n",
        "We want to try adding some of these to our tests to see how they impact the number of tests needed to cover every possible case."
      ],
      "metadata": {
        "id": "e9AS67Oo1kR_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In order to add a new input variable to this test, we add a new array for that variable containing all the variations of inputs we want to test with.\n",
        "\n",
        "I've added a new array for color with grades D through F.\n",
        "\n",
        "* [ ] Run the test. It will fail.\n",
        "* [ ] Look at the `received.txt` file. How many tests were added?\n",
        "* [ ] Once you are happy with your new file, run the cell below the test case to approve it.\n",
        "* [ ] Experiment. Change the test to add more variables, or add more variations to those variables. How does this impact your total number of tests?\n",
        "\n",
        "**CAUTION:** Be careful to only add input variations 2 or 3 at a time, so that you can see changes in the number of tests.\n",
        "\n",
        " Generating hundreds of thousands of test cases all at once might be fun, but it takes a lot of time.\n"
      ],
      "metadata": {
        "id": "N63sxp1W5cpL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%ipytest -qq\n",
        "from diamond_pricing import replace_null_prices_with_floating_averages\n",
        "from notebook_verification_helpers import verify_will_replace_null_values_with_floating_averages\n",
        "\n",
        "def test_will_replace_null_prices_with_floating_averages(spark: SparkSession) -> None:\n",
        "    price = [327, None]\n",
        "    color = [\"D\", \"E\", \"F\"]\n",
        "\n",
        "    verify_will_replace_null_values_with_floating_averages(spark, price=price, color=color)\n"
      ],
      "metadata": {
        "id": "uuu9p0-y7UiE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run the cell below to approve the results."
      ],
      "metadata": {
        "id": "jEjoew6I75Cn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mv /content/test_will_replace_null_prices_with_floating_averages.received.txt /content/test_will_replace_null_prices_with_floating_averages.approved.txt"
      ],
      "metadata": {
        "id": "3x-ExelH7pMi"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}