{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO76agARFCxkYnPf5o/GKXs",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jmasonlee/efficiently_testing_etl_pipelines/blob/main/fixing_a_big_test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup Notebook"
      ],
      "metadata": {
        "id": "aqYjZpizVD3i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf efficiently_testing_etl_pipelines\n",
        "!git clone https://github.com/jmasonlee/efficiently_testing_etl_pipelines.git\n",
        "!cp -r /content/efficiently_testing_etl_pipelines/src/ .\n",
        "!cp -r /content/efficiently_testing_etl_pipelines/tests/ .\n",
        "!rm -rf efficiently_testing_etl_pipelines\n",
        "!rm -rf tests/diamond_pricing_test*\n",
        "!rm -rf tests/test_helpers/*verification_helpers.py\n",
        "!rm -rf tests/conftest.py\n",
        "!rm -rf sample_data\n"
      ],
      "metadata": {
        "id": "i4KRAp6vTEJm",
        "outputId": "156a2a57-b36a-4e89-ce36-7e908c25eaec",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'efficiently_testing_etl_pipelines'...\n",
            "remote: Enumerating objects: 525, done.\u001b[K\n",
            "remote: Counting objects: 100% (166/166), done.\u001b[K\n",
            "remote: Compressing objects: 100% (92/92), done.\u001b[K\n",
            "remote: Total 525 (delta 107), reused 113 (delta 70), pack-reused 359\u001b[K\n",
            "Receiving objects: 100% (525/525), 239.28 KiB | 4.20 MiB/s, done.\n",
            "Resolving deltas: 100% (313/313), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup Tests"
      ],
      "metadata": {
        "id": "5cRRMTtEBYy0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Install Dependencies\n",
        "\n",
        "For the exercise, we will need some special dependencies to allow us to run lots of tests in a notebook."
      ],
      "metadata": {
        "id": "_h199h_XEAwT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "`ipytest` lets us run our tests in a notebook.\n",
        "\n"
      ],
      "metadata": {
        "id": "DVAcSonZmslt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ipytest"
      ],
      "metadata": {
        "id": "7v0kaWulDXi-",
        "outputId": "a5253c04-68a3-4f19-d1d2-dea52656aa9b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting ipytest\n",
            "  Downloading ipytest-0.13.3-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: ipython in /usr/local/lib/python3.10/dist-packages (from ipytest) (7.34.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from ipytest) (23.1)\n",
            "Requirement already satisfied: pytest>=5.4 in /usr/local/lib/python3.10/dist-packages (from ipytest) (7.2.2)\n",
            "Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.10/dist-packages (from pytest>=5.4->ipytest) (23.1.0)\n",
            "Requirement already satisfied: iniconfig in /usr/local/lib/python3.10/dist-packages (from pytest>=5.4->ipytest) (2.0.0)\n",
            "Requirement already satisfied: pluggy<2.0,>=0.12 in /usr/local/lib/python3.10/dist-packages (from pytest>=5.4->ipytest) (1.0.0)\n",
            "Requirement already satisfied: exceptiongroup>=1.0.0rc8 in /usr/local/lib/python3.10/dist-packages (from pytest>=5.4->ipytest) (1.1.1)\n",
            "Requirement already satisfied: tomli>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from pytest>=5.4->ipytest) (2.0.1)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.10/dist-packages (from ipython->ipytest) (67.7.2)\n",
            "Collecting jedi>=0.16 (from ipython->ipytest)\n",
            "  Downloading jedi-0.18.2-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m19.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from ipython->ipytest) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.10/dist-packages (from ipython->ipytest) (0.7.5)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.10/dist-packages (from ipython->ipytest) (5.7.1)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from ipython->ipytest) (3.0.38)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.10/dist-packages (from ipython->ipytest) (2.14.0)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.10/dist-packages (from ipython->ipytest) (0.2.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/dist-packages (from ipython->ipytest) (0.1.6)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from ipython->ipytest) (4.8.0)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->ipython->ipytest) (0.8.3)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.10/dist-packages (from pexpect>4.3->ipython->ipytest) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython->ipytest) (0.2.6)\n",
            "Installing collected packages: jedi, ipytest\n",
            "Successfully installed ipytest-0.13.3 jedi-0.18.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ipytest is what allows us to run our tests in a notebook. This next cell is not needed if you are writing tests in a separate pytest file."
      ],
      "metadata": {
        "id": "3_ILjU8goCNY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import ipytest\n",
        "ipytest.autoconfig()"
      ],
      "metadata": {
        "id": "t7iwGQ4Le-Oe"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We are installing `pyspark` because it doesn't come with the base colab environment"
      ],
      "metadata": {
        "id": "HzwfHFGmm2Ca"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark"
      ],
      "metadata": {
        "id": "QNnk84AsmmFQ",
        "outputId": "d3168f9b-f763-4256-fdef-9907c69742b7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyspark\n",
            "  Downloading pyspark-3.4.0.tar.gz (310.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m310.8/310.8 MB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.4.0-py2.py3-none-any.whl size=311317130 sha256=a734f02fdb61d8c789991111397d62729fd65605874e96f19384987cb756c3ca\n",
            "  Stored in directory: /root/.cache/pip/wheels/7b/1b/4b/3363a1d04368e7ff0d408e57ff57966fcdf00583774e761327\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.4.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create a local SparkSession\n",
        "\n",
        "Normally spark runs on a bunch of executors in the cloud. Since we want our tests to be able to run on a single dev machine, we make a fixture that gives us a local spark context."
      ],
      "metadata": {
        "id": "V_OJcnSiBgUb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pytest\n",
        "from _pytest.fixtures import FixtureRequest\n",
        "from pyspark import SparkConf\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "@pytest.fixture(scope=\"session\")\n",
        "def spark(request: FixtureRequest):\n",
        "    conf = (SparkConf()\n",
        "        .setMaster(\"local\")\n",
        "        .setAppName(\"sample_pyspark_testing_starter\"))\n",
        "\n",
        "    spark = SparkSession \\\n",
        "        .builder \\\n",
        "        .config(conf=conf) \\\n",
        "        .getOrCreate()\n",
        "\n",
        "    request.addfinalizer(lambda: spark.stop())\n",
        "    return spark"
      ],
      "metadata": {
        "id": "xXPLpebJ6iI2"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create Helpers\n",
        "\n",
        "This is a helper function that retrieves our test output from the expected.json file"
      ],
      "metadata": {
        "id": "actN5s7DCF9J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "def expected_json():\n",
        "    with open(\"tests/fixtures/expected.json\") as f:\n",
        "        return json.loads(f.read())"
      ],
      "metadata": {
        "id": "VO2-3px25xEl"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The Test"
      ],
      "metadata": {
        "id": "MjwHttFB5TB7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%ipytest -qq\n",
        "from src.linear_regression_prep import transform\n",
        "from tests.test_helpers.json_helpers import create_df_from_json, data_frame_to_json\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "def test_prep_for_linear_regression(spark: SparkSession):\n",
        "    diamonds_df = create_df_from_json(\"tests/fixtures/diamonds.json\", spark)\n",
        "\n",
        "    actual_df = transform(diamonds_df)\n",
        "    assert data_frame_to_json(actual_df) == expected_json()"
      ],
      "metadata": {
        "id": "lH1x9ZOi58X7",
        "outputId": "fb502532-a93f-4381-fc4d-379f0afa909e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pyspark/pandas/__init__.py:50: UserWarning: 'PYARROW_IGNORE_TIMEZONE' environment variable was not set. It is required to set this environment variable to '1' in both driver and executor sides if you use pyarrow>=2.0.0. pandas-on-Spark will set it for you but it does not work if there is a Spark context already launched.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mF\u001b[0m\u001b[31m                                                                                            [100%]\u001b[0m\n",
            "============================================= FAILURES =============================================\n",
            "\u001b[31m\u001b[1m_________________________________ test_prep_for_linear_regression __________________________________\u001b[0m\n",
            "\n",
            "spark = <pyspark.sql.session.SparkSession object at 0x7f2152662530>\n",
            "\n",
            "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_prep_for_linear_regression\u001b[39;49;00m(spark: SparkSession):\u001b[90m\u001b[39;49;00m\n",
            "        diamonds_df = create_df_from_json(\u001b[33m\"\u001b[39;49;00m\u001b[33mtests/fixtures/diamonds.json\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, spark)\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m\u001b[39;49;00m\n",
            "        actual_df = transform(diamonds_df)\u001b[90m\u001b[39;49;00m\n",
            ">       \u001b[94massert\u001b[39;49;00m data_frame_to_json(actual_df) == expected_json()\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31mE       AssertionError: assert [{'carat': 0....G', ...}, ...] == [{'carat': 0....8', ...}, ...]\u001b[0m\n",
            "\u001b[1m\u001b[31mE         At index 0 diff: {'id': '1', 'carat': 0.23, 'clarity': 'SI2', 'color': 'E', 'price': 326.0, 'clarity_index': 3.0, 'color_index': 1.0, 'indep_vars': {'type': 0, 'size': 7, 'indices': [0, 5], 'values': [0.23, 1.0]}} != {'id': '1', 'carat': 0.23, 'clarity': 'SI2', 'color': 'E', 'price': 326.0, 'indep_vars': {'type': 0, 'size': 7, 'indices': [0, 5], 'values': [0.23, 1.0]}}\u001b[0m\n",
            "\u001b[1m\u001b[31mE         Use -v to get more diff\u001b[0m\n",
            "\n",
            "\u001b[1m\u001b[31m<ipython-input-7-7302d3c10775>\u001b[0m:10: AssertionError\n",
            "-------------------------------------- Captured stderr setup ---------------------------------------\n",
            "Setting default log level to \"WARN\".\n",
            "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
            "23/06/22 02:59:52 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
            "--------------------------------------- Captured stdout call ---------------------------------------\n",
            "[{'id': '1', 'carat': 0.23, 'clarity': 'SI2', 'color': 'E', 'price': 326.0, 'clarity_index': 3.0, 'color_index': 1.0, 'indep_vars': {'type': 0, 'size': 7, 'indices': [0, 5], 'values': [0.23, 1.0]}}, {'id': 'DI-26', 'carat': 0.21, 'clarity': 'VVS1', 'color': 'D', 'price': 2460.0, 'clarity_index': 0.0, 'color_index': 0.0, 'indep_vars': {'type': 0, 'size': 7, 'indices': [0, 1, 4], 'values': [0.21, 1.0, 1.0]}}, {'id': 'DI-26', 'carat': 0.21, 'clarity': 'VVS1', 'color': 'D', 'price': 2690.0, 'clarity_index': 0.0, 'color_index': 0.0, 'indep_vars': {'type': 0, 'size': 7, 'indices': [0, 1, 4], 'values': [0.21, 1.0, 1.0]}}, {'id': 'DI-26', 'carat': 0.21, 'clarity': 'VVS1', 'color': 'D', 'price': 2690.0, 'clarity_index': 0.0, 'color_index': 0.0, 'indep_vars': {'type': 0, 'size': 7, 'indices': [0, 1, 4], 'values': [0.21, 1.0, 1.0]}}, {'id': 'DI-27', 'carat': 0.21, 'clarity': 'VVS1', 'color': 'D', 'price': 2692.0, 'clarity_index': 0.0, 'color_index': 0.0, 'indep_vars': {'type': 0, 'size': 7, 'indices': [0, 1, 4], 'values': [0.21, 1.0, 1.0]}}, {'id': 'DI-28', 'carat': 0.21, 'clarity': 'VVS1', 'color': 'G', 'price': 2000.0, 'clarity_index': 0.0, 'color_index': 3.0, 'indep_vars': {'type': 0, 'size': 7, 'indices': [0, 1], 'values': [0.21, 1.0]}}, {'id': 'DI-30', 'carat': 0.32, 'clarity': 'I1', 'color': 'D', 'price': 345.0, 'clarity_index': 1.0, 'color_index': 0.0, 'indep_vars': {'type': 0, 'size': 7, 'indices': [0, 2, 4], 'values': [0.32, 1.0, 1.0]}}, {'id': 'minimum_inputs', 'carat': 0.23, 'clarity': 'SI1', 'color': 'F', 'clarity_index': 2.0, 'color_index': 2.0, 'indep_vars': {'type': 0, 'size': 7, 'indices': [0, 3, 6], 'values': [0.23, 1.0, 1.0]}}]\n",
            "--------------------------------------- Captured stderr call ---------------------------------------\n",
            "                                                                                                    \n",
            "\u001b[36m\u001b[1m===================================== short test summary info ======================================\u001b[0m\n",
            "\u001b[31mFAILED\u001b[0m t_e5db65222a784c859fd8a8c6a01a9d12.py::\u001b[1mtest_prep_for_linear_regression\u001b[0m - AssertionError: assert [{'carat': 0....G', ...}, ...] == [{'carat': 0....8', ...}, ...]\n"
          ]
        }
      ]
    }
  ]
}