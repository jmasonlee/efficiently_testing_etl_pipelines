{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNqt7lK8pt5gg4W85cK4XcU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jmasonlee/efficiently_testing_etl_pipelines/blob/main/fixing_a_big_test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup Notebook"
      ],
      "metadata": {
        "id": "aqYjZpizVD3i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf efficiently_testing_etl_pipelines\n",
        "!git clone https://github.com/jmasonlee/efficiently_testing_etl_pipelines.git\n",
        "!cp -r /content/efficiently_testing_etl_pipelines/src/ .\n",
        "!cp -r /content/efficiently_testing_etl_pipelines/tests/ .\n",
        "!rm -rf efficiently_testing_etl_pipelines\n",
        "!rm -rf tests/diamond_pricing_test*\n",
        "!rm -rf tests/test_helpers/*verification_helpers.py\n",
        "!rm -rf tests/conftest.py\n",
        "!rm -rf sample_data\n"
      ],
      "metadata": {
        "id": "i4KRAp6vTEJm",
        "outputId": "bdf5b893-22df-483b-d5d0-e596e7013cb7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'efficiently_testing_etl_pipelines'...\n",
            "remote: Enumerating objects: 550, done.\u001b[K\n",
            "remote: Counting objects: 100% (191/191), done.\u001b[K\n",
            "remote: Compressing objects: 100% (115/115), done.\u001b[K\n",
            "remote: Total 550 (delta 121), reused 111 (delta 70), pack-reused 359\u001b[K\n",
            "Receiving objects: 100% (550/550), 246.07 KiB | 8.48 MiB/s, done.\n",
            "Resolving deltas: 100% (327/327), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup Tests"
      ],
      "metadata": {
        "id": "5cRRMTtEBYy0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Install Dependencies\n",
        "\n",
        "For the exercise, we will need some special dependencies to allow us to run lots of tests in a notebook."
      ],
      "metadata": {
        "id": "_h199h_XEAwT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "`ipytest` lets us run our tests in a notebook.\n",
        "\n"
      ],
      "metadata": {
        "id": "DVAcSonZmslt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ipytest"
      ],
      "metadata": {
        "id": "7v0kaWulDXi-",
        "outputId": "535eb069-721b-41d3-e392-6017185a6bc5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ipytest\n",
            "  Downloading ipytest-0.13.3-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: ipython in /usr/local/lib/python3.10/dist-packages (from ipytest) (7.34.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from ipytest) (23.1)\n",
            "Requirement already satisfied: pytest>=5.4 in /usr/local/lib/python3.10/dist-packages (from ipytest) (7.2.2)\n",
            "Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.10/dist-packages (from pytest>=5.4->ipytest) (23.1.0)\n",
            "Requirement already satisfied: iniconfig in /usr/local/lib/python3.10/dist-packages (from pytest>=5.4->ipytest) (2.0.0)\n",
            "Requirement already satisfied: pluggy<2.0,>=0.12 in /usr/local/lib/python3.10/dist-packages (from pytest>=5.4->ipytest) (1.2.0)\n",
            "Requirement already satisfied: exceptiongroup>=1.0.0rc8 in /usr/local/lib/python3.10/dist-packages (from pytest>=5.4->ipytest) (1.1.1)\n",
            "Requirement already satisfied: tomli>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from pytest>=5.4->ipytest) (2.0.1)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.10/dist-packages (from ipython->ipytest) (67.7.2)\n",
            "Collecting jedi>=0.16 (from ipython->ipytest)\n",
            "  Downloading jedi-0.18.2-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m19.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from ipython->ipytest) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.10/dist-packages (from ipython->ipytest) (0.7.5)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.10/dist-packages (from ipython->ipytest) (5.7.1)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from ipython->ipytest) (3.0.38)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.10/dist-packages (from ipython->ipytest) (2.14.0)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.10/dist-packages (from ipython->ipytest) (0.2.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/dist-packages (from ipython->ipytest) (0.1.6)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from ipython->ipytest) (4.8.0)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->ipython->ipytest) (0.8.3)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.10/dist-packages (from pexpect>4.3->ipython->ipytest) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython->ipytest) (0.2.6)\n",
            "Installing collected packages: jedi, ipytest\n",
            "Successfully installed ipytest-0.13.3 jedi-0.18.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ipytest is what allows us to run our tests in a notebook. This next cell is not needed if you are writing tests in a separate pytest file."
      ],
      "metadata": {
        "id": "3_ILjU8goCNY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import ipytest\n",
        "ipytest.autoconfig()"
      ],
      "metadata": {
        "id": "t7iwGQ4Le-Oe"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We are installing `pyspark` because it doesn't come with the base colab environment"
      ],
      "metadata": {
        "id": "HzwfHFGmm2Ca"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark"
      ],
      "metadata": {
        "id": "QNnk84AsmmFQ",
        "outputId": "b3e05fd2-471c-45ba-cdfd-7138727cf1f2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.4.1.tar.gz (310.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m310.8/310.8 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.4.1-py2.py3-none-any.whl size=311285398 sha256=13b8460b2c01651b31ebdfda3c116e8d7519284d585e49e934055dee2caf3462\n",
            "  Stored in directory: /root/.cache/pip/wheels/0d/77/a3/ff2f74cc9ab41f8f594dabf0579c2a7c6de920d584206e0834\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.4.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install chispa"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r_EYg-cC1cD5",
        "outputId": "bb465641-1c45-4f2b-8736-52e709fec1eb"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting chispa\n",
            "  Downloading chispa-0.9.2-py3-none-any.whl (28 kB)\n",
            "Installing collected packages: chispa\n",
            "Successfully installed chispa-0.9.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create a local SparkSession\n",
        "\n",
        "Normally spark runs on a bunch of executors in the cloud. Since we want our tests to be able to run on a single dev machine, we make a fixture that gives us a local spark context."
      ],
      "metadata": {
        "id": "V_OJcnSiBgUb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pytest\n",
        "from _pytest.fixtures import FixtureRequest\n",
        "from pyspark import SparkConf\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "@pytest.fixture(scope=\"session\")\n",
        "def spark(request: FixtureRequest):\n",
        "    conf = (SparkConf()\n",
        "        .setMaster(\"local\")\n",
        "        .setAppName(\"sample_pyspark_testing_starter\"))\n",
        "\n",
        "    spark = SparkSession \\\n",
        "        .builder \\\n",
        "        .config(conf=conf) \\\n",
        "        .getOrCreate()\n",
        "\n",
        "    request.addfinalizer(lambda: spark.stop())\n",
        "    return spark"
      ],
      "metadata": {
        "id": "xXPLpebJ6iI2"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create Helpers\n",
        "\n",
        "This is a helper function that retrieves our test output from the expected.json file"
      ],
      "metadata": {
        "id": "actN5s7DCF9J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "def expected_json():\n",
        "    with open(\"tests/fixtures/expected.json\") as f:\n",
        "        return json.loads(f.read())"
      ],
      "metadata": {
        "id": "VO2-3px25xEl"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The Test"
      ],
      "metadata": {
        "id": "MjwHttFB5TB7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%ipytest -qq\n",
        "from src.linear_regression_prep import transform\n",
        "from tests.test_helpers.json_helpers import create_df_from_json, data_frame_to_json\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "def test_prep_for_linear_regression(spark: SparkSession):\n",
        "    diamonds_df = create_df_from_json(\"tests/fixtures/diamonds.json\", spark)\n",
        "\n",
        "    actual_df = transform(diamonds_df)\n",
        "    assert data_frame_to_json(actual_df) == expected_json()"
      ],
      "metadata": {
        "id": "lH1x9ZOi58X7",
        "outputId": "c4750b10-0079-4c8e-dbb9-4b5b30868b73",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32m.\u001b[0m\u001b[32m                                                                                            [100%]\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Make The Assert Specific"
      ],
      "metadata": {
        "id": "cbCuKHDpso8C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Right now, our test compares everything in the output dataframe to everything in a large json file. That's a lot of rows to compare and the assert is wrong anyways!\n",
        "\n",
        "Let's make this test assert on the thing we actually care about - the output price of the diamond!"
      ],
      "metadata": {
        "id": "X-xfpXMwstwN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Let's make our assert specific!\n",
        "### We can do the next step in one of 3 ways:\n",
        "#### With Chispa\n",
        "- [ ] Add these imports to the top of the cell, below the `%%ipytest -qq` line:  \n",
        "`from chispa import assert_column_equality`  \n",
        "`from pySpark.sql import functions`\n",
        "- [ ] Filter the dataframe for the unique id of the diamond we care about:  \n",
        "`actual_df=actual_df.filter(actual_df.id == 'DI-26-null-price')`\n",
        "- [ ] Create a new column in our dataframe that contains our expected price:  \n",
        "`actual_df=actual_df.withColumn('expected_price', lit(2960.0))`\n",
        "- [ ] Assert the value in the price column matches the value we want:  \n",
        "`assert_column_equality(actual_df, 'price', 'expected_price')`\n"
      ],
      "metadata": {
        "id": "jz3ygb6GxcYG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%ipytest -qq\n",
        "from src.linear_regression_prep import transform\n",
        "from tests.test_helpers.json_helpers import create_df_from_json, data_frame_to_json\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import lit\n",
        "from chispa import assert_column_equality\n",
        "\n",
        "def test_prep_for_linear_regression(spark: SparkSession):\n",
        "    diamonds_df = create_df_from_json(\"tests/fixtures/diamonds.json\", spark)\n",
        "\n",
        "    actual_df = transform(diamonds_df)\n"
      ],
      "metadata": {
        "outputId": "fa65b106-7d50-4264-883d-b314c40646ed",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QiwYRKD0yXIW"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32m.\u001b[0m\u001b[32m                                                                                            [100%]\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### With Pandas\n",
        "- [ ] import pandas:  \n",
        "`import pandas as pd`\n",
        "- [ ] Filter the dataframe for the unique id of the diamond we care about:  \n",
        "  `actual_df=actual_df.filter(actual_df.id == 'DI-26-null-price')`\n",
        "- [ ] Create your expected dataframe using Pandas:  \n",
        " `expected = pd.DataFrame(({'id': [\"DI-26-null-price\"], 'price':[2690.0] }))`\n",
        "- [ ] Select the column you care about:  \n",
        "  `actual_df=actual_df.select(['id', 'price'])\n",
        "- [ ] Assert for dataframe equality using pandas:  \n",
        "  `pd.testing.assert_frame_equal(actual_df, expected)`"
      ],
      "metadata": {
        "id": "wW0OfrWo8Qn4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%ipytest -qq\n",
        "from src.linear_regression_prep import transform\n",
        "from tests.test_helpers.json_helpers import create_df_from_json, data_frame_to_json\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import lit\n",
        "from chispa import assert_column_equality\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "def test_prep_for_linear_regression(spark: SparkSession):\n",
        "    diamonds_df = create_df_from_json(\"tests/fixtures/diamonds.json\", spark)\n",
        "\n",
        "    actual_df = transform(diamonds_df)\n"
      ],
      "metadata": {
        "outputId": "fb64b8ce-8309-4873-a2e2-aceeade45a2b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3a_ORvhl9CAd"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mF\u001b[0m\u001b[31m                                                                                            [100%]\u001b[0m\n",
            "============================================= FAILURES =============================================\n",
            "\u001b[31m\u001b[1m_________________________________ test_prep_for_linear_regression __________________________________\u001b[0m\n",
            "\n",
            "spark = <pyspark.sql.session.SparkSession object at 0x7f09a86a93f0>\n",
            "\n",
            "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_prep_for_linear_regression\u001b[39;49;00m(spark: SparkSession):\u001b[90m\u001b[39;49;00m\n",
            "        diamonds_df = create_df_from_json(\u001b[33m\"\u001b[39;49;00m\u001b[33mtests/fixtures/diamonds.json\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, spark)\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m\u001b[39;49;00m\n",
            "        actual_df = transform(diamonds_df)\u001b[90m\u001b[39;49;00m\n",
            "        actual_df=actual_df.filter(actual_df.id == \u001b[33m'\u001b[39;49;00m\u001b[33mDI-26-null-price\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
            "        expected = pd.DataFrame(({\u001b[33m'\u001b[39;49;00m\u001b[33mid\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m: [\u001b[33m\"\u001b[39;49;00m\u001b[33mDI-26-null-price\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m], \u001b[33m'\u001b[39;49;00m\u001b[33mprice\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:[\u001b[94m2690.0\u001b[39;49;00m] }))\u001b[90m\u001b[39;49;00m\n",
            "        actual_df = actual_df.select([\u001b[33m'\u001b[39;49;00m\u001b[33mid\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mprice\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]).where(actual_df.id == \u001b[33m\"\u001b[39;49;00m\u001b[33mDI-26-null-price\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
            ">       pd.testing.assert_frame_equal(actual_df.toPandas(), expected)\u001b[90m\u001b[39;49;00m\n",
            "\n",
            "\u001b[1m\u001b[31m<ipython-input-13-0be942c8d32b>\u001b[0m:17: \n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\u001b[1m\u001b[31mpandas/_libs/testing.pyx\u001b[0m:52: in pandas._libs.testing.assert_almost_equal\n",
            "    \u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\n",
            ">   \u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[04m\u001b[91m?\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31mE   AssertionError: DataFrame.iloc[:, 1] (column name=\"price\") are different\u001b[0m\n",
            "\u001b[1m\u001b[31mE   \u001b[0m\n",
            "\u001b[1m\u001b[31mE   DataFrame.iloc[:, 1] (column name=\"price\") values are different (100.0 %)\u001b[0m\n",
            "\u001b[1m\u001b[31mE   [index]: [0]\u001b[0m\n",
            "\u001b[1m\u001b[31mE   [left]:  [2460.0]\u001b[0m\n",
            "\u001b[1m\u001b[31mE   [right]: [2690.0]\u001b[0m\n",
            "\n",
            "\u001b[1m\u001b[31mpandas/_libs/testing.pyx\u001b[0m:167: AssertionError\n",
            "\u001b[36m\u001b[1m===================================== short test summary info ======================================\u001b[0m\n",
            "\u001b[31mFAILED\u001b[0m t_5f8a68b7935441de8636ef9c39afbe88.py::\u001b[1mtest_prep_for_linear_regression\u001b[0m - AssertionError: DataFrame.iloc[:, 1] (column name=\"price\") are different\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Assert on properties\n",
        "\n",
        "- [ ] Filter the dataframe for the unique id of the diamond we care about:  \n",
        "  `actual_df=actual_df.filter(actual_df.id == 'DI-26-null-price')`\n",
        "- [ ] Convert your dataframe to JSON:  \n",
        "`actual_df_json = data_frame_to_json(actual_df)`\n",
        "- [ ] Assert the price property of the first object matches your expected price:  \n",
        "`assert actual_df_json[0]['price'] == 2690.0`"
      ],
      "metadata": {
        "id": "lD-3MVVv9D9d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%ipytest -qq\n",
        "from src.linear_regression_prep import transform\n",
        "from tests.test_helpers.json_helpers import create_df_from_json, data_frame_to_json\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import lit\n",
        "from chispa import assert_column_equality\n",
        "\n",
        "def test_prep_for_linear_regression(spark: SparkSession):\n",
        "    diamonds_df = create_df_from_json(\"tests/fixtures/diamonds.json\", spark)\n",
        "\n",
        "    actual_df = transform(diamonds_df)\n",
        "\n"
      ],
      "metadata": {
        "outputId": "d3700678-4a25-45ad-c2aa-5f9f2cee7152",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "55eVJ0Ug9Pif"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mF\u001b[0m\u001b[31m                                                                                            [100%]\u001b[0m\n",
            "============================================= FAILURES =============================================\n",
            "\u001b[31m\u001b[1m_________________________________ test_prep_for_linear_regression __________________________________\u001b[0m\n",
            "\n",
            "spark = <pyspark.sql.session.SparkSession object at 0x7f09a86c1c60>\n",
            "\n",
            "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_prep_for_linear_regression\u001b[39;49;00m(spark: SparkSession):\u001b[90m\u001b[39;49;00m\n",
            "        diamonds_df = create_df_from_json(\u001b[33m\"\u001b[39;49;00m\u001b[33mtests/fixtures/diamonds.json\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, spark)\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m\u001b[39;49;00m\n",
            "        actual_df = transform(diamonds_df)\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m\u001b[39;49;00m\n",
            "        actual_df=actual_df.filter(actual_df.id == \u001b[33m'\u001b[39;49;00m\u001b[33mDI-26-null-price\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
            "        actual_df_json = data_frame_to_json(actual_df)\u001b[90m\u001b[39;49;00m\n",
            ">       \u001b[94massert\u001b[39;49;00m actual_df_json[\u001b[94m0\u001b[39;49;00m][\u001b[33m'\u001b[39;49;00m\u001b[33mprice\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m] == \u001b[94m2690.0\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31mE       assert 2460.0 == 2690.0\u001b[0m\n",
            "\n",
            "\u001b[1m\u001b[31m<ipython-input-14-e71ba456c136>\u001b[0m:15: AssertionError\n",
            "--------------------------------------- Captured stdout call ---------------------------------------\n",
            "[{'id': 'DI-26-null-price', 'carat': 0.21, 'clarity': 'VVS1', 'color': 'D', 'price': 2460.0, 'clarity_index': 0.0, 'color_index': 0.0, 'indep_vars': {'type': 0, 'size': 7, 'indices': [0, 1, 4], 'values': [0.21, 1.0, 1.0]}}]\n",
            "\u001b[36m\u001b[1m===================================== short test summary info ======================================\u001b[0m\n",
            "\u001b[31mFAILED\u001b[0m t_5f8a68b7935441de8636ef9c39afbe88.py::\u001b[1mtest_prep_for_linear_regression\u001b[0m - assert 2460.0 == 2690.0\n"
          ]
        }
      ]
    }
  ]
}